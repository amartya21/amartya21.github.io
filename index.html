<!DOCTYPE html>
<html>
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-52138338-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-52138338-2');
    </script>

    <meta name="generator" content="HTML Tidy">
    <link href="stylesheets/style.css" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic" rel="stylesheet" type="text/css">
    <link rel="icon" type="image/png" href="images/umd_seal.jpg">
    <script src="js/hidebib.js" type="text/javascript"></script>

    <meta name="description" content="Graduate student in Computer Science">

    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>Amartya Banerjee</title>
  </head>
  <body>
    <table width="840" border="0" align="center" cellspacing="0" cellpadding="20">
      <tr>
        <td>
          <table width="100%" align="center" cellspacing="0" cellpadding="20">
            <p align="center">
              <font size="7">Amartya Banerjee</font>
              <br>
              <b>Email </b>
              <font id="email" style="display:inline;">amartya1@cs.unc.edu</font>
            </p>
            <td width="65%" valign="middle" align="justify">
              <p>
                I am currently a PhD student in the <a href="https://cs.unc.edu/">Department of Computer Science</a> at
                <a href="https://www.unc.edu/">University of North Carolina, Chapel Hill</a>. I am extremely fortunate to be advised by  
                <a href="https://harlinlee.github.io/"> Prof. Harlin Lee </a> and 
                <a href="https://math.unc.edu/faculty-member/moosmueller-caroline/"> Prof. Caroline Moosmueller. </a> 
                I am also a member of <a href="https://tarheels.live/cmoosm/"> Geometric Data Analysis @ UNC</a>.
                Broadly, my research interests revolve around problems in Optimal Transport and Machine Learning.
              </p>
              <p>
                Prior to joining my PhD, I graduated with a double major in Mathematics and
                Computer Science from <a href="https://www.umd.edu/">University of Maryland, College Park</a> 
                followed by a Master's in <a href="https://www.cs.umd.edu/">Computer Science</a>, also from UMD.
              </p>
              <p>Fun Fact: I have an Erdos number of 3. </p>
              <p align="center">
                <a href="https://scholar.google.com/citations?user=3ZItW_UAAAAJ&hl=en">Google Scholar</a> | 
                <a href="https://github.com/amartya21">Github</a> | 
                <a href="https://www.linkedin.com/in/amartya1">LinkedIn</a>
              </p>
            </td>
            <td width="35%" valign="top">
              <a href="images/amartya.png">
                <img src="images/amartya.png" width="100%" alt="Amartya Banerjee">
              </a>
            </td>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td>
                <sectionheading id="activities">News and Past Activities</sectionheading>

                <div style="width: 100%; max-height: 150px; overflow-y: scroll;">
                  <ul>
                    <li>[Feb 2025]: Columbia University Optimal Transport and Statistics Workshop Travel Award</li>
                    <li>[Jan 2025]: Invited Talk at IIT Bombay</li>
                    <li>[Dec 2024]: First Place in Poster &amp; Short Talks Competition at <a href="https://sites.google.com/view/data-science-week-2024/posters-and-short-talks" target="_blank">Data Science Week</a>, Purdue University</li>
                    <li>[Oct 2024]: Our work on trajectory inference was covered by <a href="https://www.siam.org/publications/siam-news/articles/novel-algorithm-infers-particle-trajectories-in-cell-point-clouds/" target="_blank">SIAM News</a></li>
                    <li>[Oct 2024]: Poster presentation prize at <a href="https://tarheels.live/cmoosm/2024/10/04/unc-data-science-day-poster-presentations/" target="_blank">SDSS Data Science Day</a></li>
                    <li>[Aug 2024]: SIAM MDS24 Travel Award</li>
                    <li>[Nov 2023]: TriCAMS at Duke University</li>
                  </ul>
                </div>
              </td>
            </tr>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td>
                <sectionheading>Research</sectionheading>
              </td>
            </tr>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <!-- -------------------------------------------------------------------------->
            <tr>
              <td>
                <h2> <font color="gray">2025</font> </h2>
              </td>
            </tr>
            <!-- -------------------------------------------------------------------------->

            <tr>
              <td width="30%" valign="top" align="center">
                <a href="https://arxiv.org/pdf/2405.19679">
                  <img src="images/teaser_fig/wlr_eg.png" alt="sym" width="100%" style="border-style: none" />
                </a>
              </td>
              <td width="70%" valign="top">
                <a href="https://arxiv.org/pdf/2405.19679" id="2025_aistats_paper">
                  <paper_title>Efficient Trajectory Inference in Wasserstein Space Using Consecutive Averaging</paper_title>
                </a>
                <br>
                <strong>Amartya Banerjee</strong>, Harlin Lee, Nir Sharon, Caroline Moosm&uuml;ller.
                <br>  <em>AISTATS</em>, 2025
                <br>
                <div class="paper" id="2025_aistats">
                  <a href="https://arxiv.org/pdf/2405.19679">pdf</a>
                  | <a href="javascript:toggleblock('2025_wlr_abs')">abstract</a>
                  | <a shape="rect" class="togglebib" href="javascript:togglebib('2025_aistats')">bibtex</a>
                  | <a href="https://github.com/amartya21/Wasserstein-Trajectory-Inference">Code</a>
                  | <a href="https://www.siam.org/publications/siam-news/articles/novel-algorithm-infers-particle-trajectories-in-cell-point-clouds/">SIAM News</a>
                  <p align="justify"><i id="2025_wlr_abs">
                    Capturing data from dynamic processes through cross-sectional measurements is seen in many fields, such as computational biology. Trajectory 
                    inference deals with the challenge of reconstructing continuous processes from such observations. In this work, we propose methods for B-spline 
                    approximation and interpolation of point clouds through consecutive averaging that is intrinsic to the Wasserstein space. Combining subdivision 
                    schemes with optimal transportbased geodesic, our methods carry out trajectory inference at a chosen level of precision and smoothness, and can 
                    automatically handle scenarios where particles undergo division over time. We prove linear convergence rates and rigorously evaluate our method 
                    on cell data characterized by bifurcations, merges, and trajectory splitting scenarios like supercells, comparing its performance against 
                    state-of-the-art trajectory inference and interpolation methods. The results not only underscore the effectiveness of our method in inferring 
                    trajectories but also highlight the benefit of performing interpolation and approximation that respect the inherent geometric properties of the 
                    data.
                  </i></p>
                  <pre xml:space="preserve">
@inproceedings{
banerjee2025efficient,
title={Efficient Trajectory Inference in Wasserstein Space Using Consecutive Averaging},
author={Amartya Banerjee and Harlin Lee and Nir Sharon and Caroline Moosm{\"u}ller},
booktitle={The 28th International Conference on Artificial Intelligence and Statistics},
year={2025},
url={https://openreview.net/forum?id=ucyKTM7lO5}
}
                  </pre>
                </div>
              </td>
            </tr>

            <!-- -------------------------------------------------------------------------->
            <tr>
              <td>
                <h2> <font color="gray">2024</font> </h2>
              </td>
            </tr>
            <!-- -------------------------------------------------------------------------->

            <tr>
              <td width="30%" valign="top" align="center">
                <a href="https://arxiv.org/pdf/2311.10246.pdf">
                  <img src="images/teaser_fig/howso_conviction_eg.png" alt="sym" width="100%" style="border-style: none" />
                </a>
              </td>
              <td width="70%" valign="top">
                <a href="https://arxiv.org/pdf/2311.10246.pdf" id="2024_howso_paper">
                  <paper_title>Surprisal Driven K-NN for Robust and Interpretable Nonparametric Learning</paper_title>
                </a>
                <br>
                <strong>Amartya Banerjee</strong>, Christopher J. Hazard, Jacob Beel, Cade Mack, Jack Xia, Michael Resnick, Will Goddin.
                <br>  <em>arXiv</em>, 2024
                <br>
                <div class="paper" id="2024_howso">
                  <a href="https://arxiv.org/pdf/2311.10246.pdf">pdf</a>
                  | <a href="javascript:toggleblock('2024_howso_abs')">abstract</a>
                  | <a shape="rect" class="togglebib" href="javascript:togglebib('2024_howso')">bibtex</a>
                  | <a href="https://github.com/howsoai/howso-engine">Code</a>
                  <p align="justify"><i id="2024_howso_abs">
                    Nonparametric learning is a fundamental concept in machine learning that aims to capture complex patterns and relationships in data without making strong assumptions about the underlying data distribution. Owing to simplicity and familiarity, one of the most well-known algorithms under this paradigm is the k-nearest neighbors (k-NN) algorithm. Driven by the usage of machine learning in safety-critical applications, in this work, we shed new light on the traditional nearest neighbors algorithm from the perspective of information theory and propose a robust and interpretable framework for tasks such as classification, regression, density estimation, and anomaly detection using a single model. We can determine data point weights as well as feature contributions by calculating the conditional entropy for adding a feature without the need for explicit model training. This allows us to compute feature contributions by providing detailed data point influence weights with perfect attribution and can be used to query counterfactuals. Instead of using a traditional distance measure which needs to be scaled and contextualized, we use a novel formulation of surprisal (amount of information required to explain the difference between the observed and expected result). 
                    Finally, our work showcases the architecture's versatility by achieving state-of-the-art results in classification and anomaly detection, while also attaining competitive results for regression across a statistically significant number of datasets.
                  </i></p>
                  <pre xml:space="preserve">
@misc{banerjee2024surprisal,
  title={Surprisal Driven $k$-NN for Robust and Interpretable Nonparametric Learning}, 
  author={Amartya Banerjee and Christopher J. Hazard and Jacob Beel and Cade Mack and Jack Xia and Michael Resnick and Will Goddin},
  year={2024},
  eprint={2311.10246},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}
                  </pre>
                </div>
              </td>
            </tr>

            <tr>
              <td width="30%" valign="top" align="center">
                <a href="https://arxiv.org/pdf/2308.06382.pdf">
                  <img src="images/teaser_fig/phenome_hallucinator.png" alt="sym" width="100%" style="border-style: none" />
                </a>
              </td>
              <td width="70%" valign="top">
                <a href="https://arxiv.org/pdf/2308.06382.pdf" id="2024_aaai_paper">
                  <paper_title>Phoneme Hallucinator: One-shot Voice Conversion via Set Expansion</paper_title>
                </a>
                <br>
                Siyuan Shan, Yang Li, <strong>Amartya Banerjee</strong>, Junier B. Oliva.
                <br>  <em>AAAI</em>, 2024
                <br>
                <div class="paper" id="2024_aaai">
                  <a href="https://arxiv.org/pdf/2308.06382.pdf">pdf</a>
                  | <a href="javascript:toggleblock('2024_aaai_abs')">abstract</a>
                  | <a shape="rect" class="togglebib" href="javascript:togglebib('2024_aaai')">bibtex</a>
                  | <a href="https://github.com/PhonemeHallucinator/Phoneme_Hallucinator">Code</a>
                  <p align="justify"><i id="2024_aaai_abs">
                    Voice conversion (VC) aims at altering a person's voice to make it sound similar to the voice of another person while preserving linguistic content. Existing methods suffer from a dilemma between content intelligibility and speaker similarity; i.e., methods with higher intelligibility usually have a lower speaker similarity, while methods with higher speaker similarity usually require plenty of target speaker voice data to achieve high intelligibility. In this work, we propose a novel method <i>Phoneme Hallucinator</i> that achieves the best of both worlds. Phoneme Hallucinator is a one-shot VC model; it adopts a novel model to hallucinate diversified and high-fidelity target speaker phonemes based just on a short target speaker voice (e.g. 3 seconds). The hallucinated phonemes are then exploited to perform neighbor-based voice conversion. Our model is a text-free, any-to-any VC model that requires no text annotations and supports conversion to any unseen speaker. Quantitative and qualitative evaluations show that <i>Phoneme Hallucinator</i> outperforms existing VC methods for both intelligibility and speaker similarity. 
                  </i></p>
                  <pre xml:space="preserve">
@article{DBLP:journals/corr/abs-2308-06382,
  author       = {Siyuan Shan and
                  Yang Li and
                  Amartya Banerjee and
                  Junier B. Oliva},
  title        = {Phoneme Hallucinator: One-shot Voice Conversion via Set Expansion},
  journal      = {CoRR},
  volume       = {abs/2308.06382},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2308.06382},
  doi          = {10.48550/ARXIV.2308.06382},
  eprinttype   = {arXiv},
  eprint       = {2308.06382},
  keywords     = {Voice Conversion, Set Expansion}
}
                  </pre>
                </div>
              </td>
            </tr>

            <!-- -------------------------------------------------------------------------->
            <tr>
              <td>
                <h2> <font color="gray">2021</font> </h2>
              </td>
            </tr>
            <!-- -------------------------------------------------------------------------->

            <tr>
              <td width="30%" valign="top" align="center">
                <a href="https://arxiv.org/pdf/2008.08183.pdf">
                  <img src="images/teaser_fig/kneser_ex.png" alt="sym" width="100%" style="border-style: none" />
                </a>
              </td>
              <td width="70%" valign="top">
                <a href="https://arxiv.org/pdf/2008.08183.pdf" id="2021ICLRoverparam_paper">
                  <paper_title>Toughness of Kneser Graphs</paper_title>
                </a>
                <br>
                Davin Park, Anthony Ostuni, Nathan Hayes, <strong>Amartya Banerjee</strong>, 
                Tanay Wakhare, Wiseley Wong, and Sebastian Cioab&atilde;.
                <br>  
                <em>Discrete Mathematics</em>, 2021
                <br>
                <div class="paper" id="2021ICLRoverparam">
                  <a href="https://arxiv.org/pdf/2008.08183.pdf">pdf</a>
                  | <a href="javascript:toggleblock('2021DiscreteMath_abs')">abstract</a>
                  | <a shape="rect" class="togglebib" href="javascript:togglebib('2021ICLRoverparam')">bibtex</a>
                  | <a href="https://github.com/aostuni/kneser-toughness">Code</a>
                  <p align="justify"><i id="2021DiscreteMath_abs">
                    The toughness t(G) of a graph G is a measure of its connectivity that is closely related to Hamiltonicity.
                    Xiaofeng Gu, confirming a longstanding conjecture of Brouwer, recently proved the lower bound t(G) ≥ ℓ/λ−1 
                    on the toughness of any connected ℓ-regular graph, where λ is the largest nontrivial absolute eigenvalue 
                    of the adjacency matrix. ...
                  </i></p>
                  <pre xml:space="preserve">
@article{PARK2021112484,
  title     = {The toughness of Kneser graphs},
  journal   = {Discrete Mathematics},
  volume    = {344},
  number    = {9},
  pages     = {112484},
  year      = {2021},
  issn      = {0012-365X},
  doi       = {https://doi.org/10.1016/j.disc.2021.112484},
  author    = {Davin Park and Anthony Ostuni and Nathan Hayes and
               Amartya Banerjee and Tanay Wakhare and Wiseley Wong 
               and Sebastian Cioabă},
  keywords  = {Kneser graph, Graph connectivity, Toughness},
}
                  </pre>
                </div>
              </td>
            </tr>
          </table>

          <table width="100%" align="center" border="0" cellpadding="5">
            <tr>
              <td>
                <sectionheading>Professional Experience</sectionheading>
                <ul>
                  <li><a href="https://www.cmu.edu/">Carnegie Mellon University, Pittsburgh</a> (Research Intern, Summer 2024)</li>
                  <li><a href="https://www.howso.com/">Howso (Formerly Diveplane)</a> (Research Intern, Summer 2023) 
                    <br>Topic: Anomaly Detection
                  </li>
                </ul>
              </td>
            </tr>
          </table>

          <table width="100%" align="center" border="0" cellpadding="5">
            <tr>
              <td>
                <sectionheading>Selected Awards</sectionheading>
                <ul>
                  <li><b>Columbia University Optimal Transport and Statistics Workshop Travel Award</b> (2025)</li>
                  <li><b>SDSS Data Science Day Poster Presentation Prize</b> (2024)</li>
                  <li><b>SIAM MDS24 Travel Award</b> (2024)</li>
                  <li><b>John D. Gannon Endowed Scholarship</b> (2018): Only international student to receive this merit-based scholarship
                    for the academic year.
                  </li>
                  <li><b>Now-A-Terp Scholarship</b> (2017)</li>
                  <li><b>GMU Merit Scholarship</b> (2016)</li>
                </ul>
              </td>
            </tr>
          </table>
        </td>
      </tr>
    </table>

    <!-- Hide all bibs and abstracts on load: -->
    <script>hideallbibs();</script>
    <script>hideblock('2021DiscreteMath_abs');</script>
    <script>hideblock('2024_aaai_abs');</script>
    <script>hideblock('2024_howso_abs');</script>
    <script>hideblock('2025_wlr_abs');</script>
  </body>
</html>
